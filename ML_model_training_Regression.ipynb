{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2593f970-404a-4c72-9750-0c1251e8624d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Enables autoreload; learn more at https://docs.databricks.com/en/files/workspace-modules.html#autoreload-for-python-modules\n",
    "# To disable autoreload; run %autoreload 0\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31cd8be-7129-4ae3-87be-e89960e42ace",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c8e039-fcc5-4451-8aea-8d89f25f201b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import threadpoolctl\n",
    "threadpoolctl.threadpool_info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569546c7-0184-48ea-940f-fd64abca8700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import preprocessing\n",
    "from preprocessing import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfcd5e2-9d3f-4dfd-8250-a86a39e1843f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data= pd.read_csv(\"emi_prediction_dataset.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "513cad9a-3e0f-41c8-a4f9-ef3b513ca382",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a4c06f-2cf5-4313-af75-03cc3229361b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pre=quality_check(data, Prediction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37295bdd-f77d-440b-8306-7415c6d21b02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data = pre.final_processsing().emi_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5cbf82a-c8f9-40dc-a846-ea9ce69f86b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e685b0c1-b3e9-4aa5-9bf7-a8ec33cda9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18242b59-e0dc-40e3-a2f4-e55e10b909bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe2b048-1f52-4662-9c24-01ac82b710c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c745f3a8-1eec-4823-b451-d7b6c9f44d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_cols=['emi_eligibility', 'max_monthly_emi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a696ca8a-8c38-41f6-b145-d15aceb4856a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de28ea67-9461-4df3-b28f-919cfeb1aae2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef, precision_recall_fscore_support, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27c7940-108c-4761-8444-36eb10b542ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_prepare(data, datatype=\"type1\", target_type=None):\n",
    "    final_columns_v1 = [\n",
    "    'gender', 'marital_status', 'education', 'monthly_salary',\n",
    "       'employment_type', 'years_of_employment', 'company_type', 'house_type',\n",
    "       'monthly_rent', 'family_size', 'dependents', 'school_fees',\n",
    "       'college_fees', 'travel_expenses', 'groceries_utilities',\n",
    "       'other_monthly_expenses', 'existing_loans', 'current_emi_amount',\n",
    "       'credit_score', 'bank_balance', 'emergency_fund', 'emi_scenario',\n",
    "       'requested_amount', 'requested_tenure']\n",
    "    final_columns_v2=[\n",
    "     'gender', 'marital_status', 'education', 'monthly_salary',\n",
    "       'employment_type', 'years_of_employment', 'company_type', 'house_type',\n",
    "       'monthly_rent', 'family_size', 'dependents',  'existing_loans', 'credit_score', 'emi_scenario',\n",
    "       'requested_amount', 'requested_tenure', 'Total_Fixed_expenses',\n",
    "       'Total_Other_expenses','Savings']\n",
    "    if datatype==\"type1\":\n",
    "        if target_type==None:\n",
    "            final_data=data[final_columns_v1]\n",
    "        elif target_type==\"regression\":\n",
    "            final_data=data[final_columns_v1]\n",
    "            final_data['max_monthly_emi']=data[\"max_monthly_emi\"]\n",
    "        elif target_type==\"classification\":\n",
    "            final_data=data[final_columns_v1]\n",
    "            final_data['emi_eligibility']=data[\"emi_eligibility\"]\n",
    "        else:\n",
    "            print(\"Please enter correct target type\")\n",
    "    elif datatype==\"type2\":\n",
    "        if target_type==None:\n",
    "            final_data=data[final_columns_v2]\n",
    "        elif target_type==\"regression\":\n",
    "            final_data=data[final_columns_v2]\n",
    "            final_data['max_monthly_emi']=data[\"max_monthly_emi\"]\n",
    "        elif target_type==\"classification\":\n",
    "            final_data=data[final_columns_v2]\n",
    "            final_data['emi_eligibility']=data[\"emi_eligibility\"]\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480909da-e212-4f61-8733-08cce231c383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2a2b31-bff7-40b3-b34b-17dd21477c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "def data_split(cdata, rebalance = False):\n",
    "    X=cdata.drop(\"emi_eligibility\", axis=1)\n",
    "    y=cdata[\"emi_eligibility\"]\n",
    "    if rebalance==True:\n",
    "        sm = SMOTE(random_state=42)\n",
    "        X, y = sm.fit_resample(X, y)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40f39d61-8f6a-48e8-95a5-eac8e94bbf28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "850d4729-d424-4811-a6f4-3def74b9a818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02db6551-c77f-403d-8152-ff371aeb5263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b9579d5-e0f8-424f-bb3f-47dca1c3b254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51e2e11c-897a-413c-adaa-08162f75825d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8bd1e1a-eedc-4d9b-b0ca-3ede90b73b1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37df5e10-0e2a-4415-a4ab-85a8c79243f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:    mlflow.create_experiment(\"/Workspace/Users/sushantkashikar1@gmail.com/mlflow/RandomForestRegressor_Optuna\")\n",
    "except Exception as e:\n",
    "    if 'RESOURCE_ALREADY_EXISTS' in str(e):\n",
    "        experiment_id = mlflow.get_experiment_by_name(\"/Workspace/Users/sushantkashikar1@gmail.com/mlflow/RandomForestRegressor_Optuna\").experiment_id\n",
    "    else:\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed27a0e1-b869-44f9-8e1b-88f92da482c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    explained_variance_score,\n",
    "    median_absolute_error\n",
    ")\n",
    "\n",
    "# Load dataset (replace with your actual EMI dataset)\n",
    "final_data = data_prepare(n_data, datatype=\"type1\", target_type=\"regression\")\n",
    "X=final_data.drop([target_cols[1]], axis=1)\n",
    "y=final_data[target_cols[1]]\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"/Workspace/Users/sushantkashikar1@gmail.com/mlflow/RandomForestRegressor_Optuna\")\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Hyperparameters to tune\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 300, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 5)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "\n",
    "        # Model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Log parameters & metric to MLflow\n",
    "        mlflow.log_params(trial.params)\n",
    "        #mlflow.log_metric(\"cv_RMSE\", rmse)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evs = explained_variance_score(y_test, y_pred)\n",
    "        medae = median_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        std_error = np.std(y_test - y_pred)\n",
    "\n",
    "        # Log parameters and metrics for the trial\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"MAPE(%)\": mape,\n",
    "            \"R2\": r2,\n",
    "            \"ExplainedVariance\": evs,\n",
    "            \"MedianAE\": medae,\n",
    "            \"MSE\": mse,\n",
    "            \"Std_Error\": std_error\n",
    "        })\n",
    "\n",
    "        return rmse\n",
    "\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10, n_jobs=1)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Log final model\n",
    "with mlflow.start_run(run_name=\"Best_RF_Model\"):\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    sign=mlflow.models.infer_signature(X_train, best_model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(best_model, \"model\", signature=sign)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afef3417-6543-49a1-9e8a-c91196a0c6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    explained_variance_score,\n",
    "    median_absolute_error\n",
    ")\n",
    "\n",
    "with open(\"assets/v1_scaler.pkl\", \"rb\") as f:\n",
    "    v1_scaler = pickle.load(f)\n",
    "with open(\"assets/v2_scaler.pkl\", \"rb\") as f:\n",
    "    v2_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# Load dataset (replace with your actual EMI dataset)\n",
    "final_data = data_prepare(n_data, datatype=\"type1\", target_type=\"regression\")\n",
    "X=final_data.drop([target_cols[1]], axis=1)\n",
    "y=final_data[target_cols[1]]\n",
    "\n",
    "X= pd.DataFrame(v1_scaler.transform(X), columns=X.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"/Workspace/Users/sushantkashikar1@gmail.com/mlflow/RandomForestRegressor_Optuna\")\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Hyperparameters to tune\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 300, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 4, 15)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 3, 15)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "\n",
    "        # Model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Cross-validation\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Log parameters & metric to MLflow\n",
    "        mlflow.log_params(trial.params)\n",
    "        #mlflow.log_metric(\"cv_RMSE\", rmse)\n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evs = explained_variance_score(y_test, y_pred)\n",
    "        medae = median_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        std_error = np.std(y_test - y_pred)\n",
    "\n",
    "        # Log parameters and metrics for the trial\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"MAPE(%)\": mape,\n",
    "            \"R2\": r2,\n",
    "            \"ExplainedVariance\": evs,\n",
    "            \"MedianAE\": medae,\n",
    "            \"MSE\": mse,\n",
    "            \"Std_Error\": std_error\n",
    "        })\n",
    "\n",
    "        return rmse\n",
    "\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10, n_jobs=1)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Log final model\n",
    "with mlflow.start_run(run_name=\"Best_RF_Model\"):\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    sign=mlflow.models.infer_signature(X_train, best_model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(best_model, \"model\", signature=sign)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a1aac7-a02e-4d99-a6b0-2b12967b597c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    explained_variance_score,\n",
    "    median_absolute_error\n",
    ")\n",
    "\n",
    "\n",
    "with open(\"assets/v1_scaler.pkl\", \"rb\") as f:\n",
    "    v1_scaler = pickle.load(f)\n",
    "with open(\"assets/v2_scaler.pkl\", \"rb\") as f:\n",
    "    v2_scaler = pickle.load(f)\n",
    "\n",
    "\n",
    "n_data[\"Savings\"] = n_data[\"bank_balance\"] + n_data[\"emergency_fund\"]\n",
    "\n",
    "# Load dataset (replace with your actual EMI dataset)\n",
    "final_data = data_prepare(n_data, datatype=\"type2\", target_type=\"regression\")\n",
    "X=final_data.drop([target_cols[1]], axis=1)\n",
    "y=final_data[target_cols[1]]\n",
    "\n",
    "X= pd.DataFrame(v2_scaler.transform(X), columns=X.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set up MLflow experiment\n",
    "mlflow.set_experiment(\"/Workspace/Users/sushantkashikar1@gmail.com/mlflow/RandomForestRegressor_Optuna\")\n",
    "\n",
    "# Define objective function for Optuna\n",
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Hyperparameters to tune\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 300, 500)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 20)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 15)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 3, 15)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "\n",
    "        # Model\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Cross-validation\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Log parameters & metric to MLflow\n",
    "        mlflow.log_params(trial.params)\n",
    "        \n",
    "        rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred) * 100\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        evs = explained_variance_score(y_test, y_pred)\n",
    "        medae = median_absolute_error(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        std_error = np.std(y_test - y_pred)\n",
    "\n",
    "        # Log parameters and metrics for the trial\n",
    "        \n",
    "        mlflow.log_metrics({\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"MAPE(%)\": mape,\n",
    "            \"R2\": r2,\n",
    "            \"ExplainedVariance\": evs,\n",
    "            \"MedianAE\": medae,\n",
    "            \"MSE\": mse,\n",
    "            \"Std_Error\": std_error\n",
    "        })\n",
    "\n",
    "        return rmse\n",
    "\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10, n_jobs=1)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best parameters:\", study.best_params)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "best_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Log final model\n",
    "with mlflow.start_run(run_name=\"Best_RF_Model\"):\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    sign=mlflow.models.infer_signature(X_train, best_model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(best_model, \"model\", signature=sign)\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_model_training_Regression",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
